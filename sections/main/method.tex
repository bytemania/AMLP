\clearpage
\section{Methodology}

This methodology follows established approaches in stock market prediction using deep
learning models, drawing from previous research in the 
field~\parencite{parmar2018stock, nabipour2020DeepLearning,
chang2024StockPrediction,agrawal2022StockPrediction, shaban2024SMPDL}. 

We present here the step-by-step process undertaken to build, train, and evaluate deep
learning models for \acrshort{wday} stock price prediction, ensuring that each methodological
decision is justified based on prior studies or logical reasoning.

\subsection{Methodology workflow}

\begin{figure}[H]
    \centering
    \caption{Workflow for Stock Market Prediction using LSTM and related models}
    \label{fig:modelwf}
    \begin{tikzpicture}[node distance=1.5cm]
        \node (dataset) [startstop] {Twelve Stock Market Dataset};
        \node (api) [process, below of=dataset] {\textbf{Fetch WDAY Stock Prices and Indicators}};
        \node (preprocessing) [process, below of=api] {\textbf{Data Preprocessing}};
        \node (learning) [process, below of=preprocessing] {\textbf{Learning Algorithms}};
        \node (hyperparam) [process, right of=learning, xshift=3.5cm] {\textbf{Hyperparameter Tuning}};
        \node (validation) [process, below of =learning] {\textbf{Cross Validation}};
        \node (performance) [process, below of=validation] {\textbf{Performance Measures}};
        \node (comparison) [process, below of=performance] {\textbf{Comparison of Results}};
    
        \draw [arrow] (dataset.south) -- (api.north);
        \draw [arrow] (api.south) -- (preprocessing.north);
        \draw [arrow] (preprocessing.south) -- (learning.north);
        \draw [arrow] (hyperparam.west) --(learning.east);
        \draw [arrow] (learning.south) -- (validation.north);
        \draw [arrow] (validation.south) -- (performance.north);
        \draw [arrow] (performance.south) -- (comparison.north);
    \end{tikzpicture}
\end{figure}

The diagram (Figure~\ref{fig:modelwf}) presents a structured workflow for stock market prediction 
using \acrshort{lstm} and related deep learning models. The workflow is divided into key stages that
ensure a systematic approach to data processing, training, evaluation, and comparison. Below is a 
step-by-step breakdown of each component:

\begin{description}
    \item[Twelve Stock Market Dataset] The process begins with collecting stock market data from
         twelve different markets. This dataset serves as the foundation for training and evaluation
    \item[Fetch WDAY Stock Prices and Indicators]  From the dataset, \acrshort{wday} stock prices 
         and technical indicators are extracted for predictive modeling.
    \item[Data Preprocessing] Raw stock price data undergoes cleaning, normalization, and 
         transformation to ensure compatibility with machine learning models.
    \item[Learning Algorithms] The \acrshort{lstm} models are trained using the processed dataset.
    \item[Hyperparameter Tuning] Hyperparameter tuning is performed in parallel to optimize
        learning rates, batch sizes, dropout rates, and activation functions for better performance.
    \item[Cross-Validation] The models are validated using cross-validation techniques to assess 
         generalization ability and reduce overfitting.     
    \item[Performance Measures] The trained models are evaluated using metrics such as 
         \acrshort{rmse}, \acrshort{mae}, \acrshort{mse}, and \acrshort{r2} to determine 
         prediction accuracy.
    \item[Comparison of Results] The final step involves comparing the performance of 
         different models and identifying the best-performing approach for stock price forecasting.
\end{description}

\subsection{Data Collection}

Stock market prediction research has demonstrated that historical price trends, combined 
with technical indicators, improve forecasting accuracy~\parencite{guo2024LSTMStock,
agrawal2022StockPrediction,balasubramanian2023SystematicSurvey,phuoc2024StockPrediction}. 

The study leverages historical stock price data for \acrshort{wday}, obtained from the
Twelve Stock Market API. This dataset includes daily open, high, low, close, and volume prices. In
addition, the study incorporates multiple technical indicators, to
enhance predictive accuracy~\parencite{parmar2018stock, nabipour2020DeepLearning,
guo2024LSTMStock}. 

We will organize our indicators into distinct categories, each designed to quantify a unique aspect that contributes to the final stock price:

\begin{description}
    \item[Pricing Data] Includes daily open, high, low, close, and volume (OHLCV) values,
    which serve as the foundation for tracking stock performance.
    \item[Trend Indicators] Measure the overall direction of the market, helping identify 
    whether a stock is in an uptrend, downtrend, or moving sideways.
    \item[Momentum Indicators] Evaluate the speed and strength of price movements, providing
    insights into potential trend reversals and whether the stock is overbought or oversold.
    \item[Volatility Indicators] Assess market risk and price fluctuations, helping traders 
    identify potential breakouts and periods of stability.
    \item[Volume Indicators] Analyze trading activity to confirm price trends, showing
    whether investors are accumulating (buying) or distributing (selling) a stock.
\end{description}

A detailed overview of the dataset is available in Appendix~\ref{app:dataset}.

\subsection{Data Preprocessing}

\subsubsection{Timeframe and frequency}

The objective of this study is to \emph{predict the next dayâ€™s closing
price of \acrfull{wday} stock}. Based on the comparative analysis in 
Table~\ref{tab:data-window}, a 10-year historical window (2014 to 2025, current date) with a daily 
interval is selected. 

\begin{landscape}
\begin{table}[H]
\centering
\caption{Recommended historical data span for different time intervals in stock price prediction using LSTM-based models.}
\label{tab:data-window}
\begin{tabular}{p{3cm}llp{3cm}p{6cm}p{6cm}} \hline
     \textbf{Study} & \textbf{Window} & \textbf{Interval} & \textbf{Use Case} & \textbf{Pros}  & \textbf{Cons} \\ \hline\hline
     \parencite{shaban2024SMPDL} & Every Minute & 2 Years & 
     \acrfull{hft} &
     High temporal precision, intraday volatility modeling, real-time insights. &
     Extremely noisy, heavy computation, large storage.
     \\
     \parencite{guo2024LSTMStock} & Every Minute & 125 days & 
     \acrfull{hft} &
      Small bursts of data for micro-movement learning. &
      Insufficient for long-range patterns; limited context.
      \\
      \parencite{chang2024StockPrediction}, 
      \parencite{nabipour2020DeepLearning} & Daily & 10 Years & 
      Daily Close Price and Medium-Term Forecasting &
      Captures full economic cycles. &
      Not usable for intraday strategies.
     \\
     \hline
\end{tabular}
\end{table}
\fillandplacepagenumber
\end{landscape}

\subsubsection{Feature transformation}

Since stock market data is inherently \emph{time-series based}, the date column must be properly transformed 
into a \emph{datetime} format and set as the dataset's \emph{index}. This transformation is crucial for 
ensuring temporal consistency and optimizing analytical procedures. Proper datetime formatting enables 
various time-series-specific operations, such as trend identification and seasonal pattern detection, which 
are fundamental for accurate forecasting models~\parencite{chang2024StockPrediction}.

By setting the date as an index, the dataset benefits in multiple ways:
\begin{description}
    \item[Chronological Integrity] Ensures that stock prices are analyzed in the correct order,
    avoiding inconsistencies in sequential learning models like LSTM and GRU~\parencite{guo2024LSTMStock}.
    \item[Enhanced Feature Extraction] Facilitates time-based calculations such as moving averages, 
    momentum, and rolling-window statistics that provide deeper insights into stock 
    trends~\parencite{shaban2024SMPDL}.
    \item[Efficient Resampling] Enables aggregation or decomposition of data into different time 
    intervals, such as hourly, daily, or weekly periods, which can be beneficial for various prediction 
    models~\parencite{agrawal2022StockPrediction}.
\end{description} 

\subsubsection{Dataset split}

In financial forecasting, time-based dataset splitting is critical to
preserving the sequential nature of stock market data. Unlike traditional
machine learning datasets, where random splitting can be applied, time-series
data must maintain chronological integrity to reflect real-world prediction
challenges. Several studies emphasize the importance of time-aware partitioning
in stock price prediction, ensuring that past data informs future forecasts
without data leakage.

For stock price forecasting, the dataset is divided as described 
in Table~\ref{tab:dataset_split}.

\begin{table}[H]
    \centering
    \caption{Time-Based Dataset Splitting Strategy}
    \label{tab:dataset_split}
    \begin{tabular}{ccp{9cm}}
        \hline
        \textbf{Dataset} & \textbf{Percentage} & \textbf{Description} \\
        \hline\hline
        Training Set & 70\% & Contains the earliest stock
        price data, allowing the model to learn long-term historical patterns. \\
        Validation Set & 15\% & Consists of the data immediately following 
        the training period, used for hyperparameter tuning. \\
        Test Set & 15\% & Represents the most recent stock prices, ensuring that 
        evaluation occurs on completely unseen future data. \\
        \hline
    \end{tabular}
\end{table}

This partitioning aligns with best practices in financial machine learning, 
where an 80/20 or 70/15/15 split balances the need for a sufficiently large 
training set while keeping a meaningful test set for 
evaluation~\parencite{chang2024StockPrediction}.

\paragraph{How the Data is Split Over Time}
As shown in Equation~\ref{eq:dataset_split}, the dataset is divided into three sequential parts to maintain chronological integrity.

Let $D={X_1,X_2,\cdots,X_T}$ be the full dataset where $X_t$ represents the stock market data at time $t$. 

\begin{equation}
\label{eq:dataset_split}
    \underbrace{X_1, X_2, \dots, X_{T_{\text{train}}}}_{\text{Training Data (70\%)}} \quad
    \underbrace{X_{T_{\text{train}}+1}, \dots, X_{T_{\text{val}}}}_{\text{Validation Data (15\%)}} \quad
    \underbrace{X_{T_{\text{val}}+1}, \dots, X_T}_{\text{Test Data (15\%)}} 
\end{equation}

This strict time-based split ensures that future data is not used to train or adjust the model, maintaining realistic forecast conditions\parencite{guo2024LSTMStock}.

\subsubsection{Feature Scaling}

Drawing from the studies summarized in Table~\ref{tab:feature-scaling}, this research evaluates both 
\emph{Min-Max normalization} and \emph{Z-score standardization} as feature scaling techniques for improving 
model performance in next-day stock price forecasting. While both methods are implemented for comparison, the 
primary scaling technique adopted in this study is Min-Max normalization, applied using the \texttt{MinMaxScaler} 
from the \texttt{scikit-learn} library. This choice is guided by evidence in the literature suggesting that 
Min-Max scaling is better suited for deep learning modelsâ€”particularly \acrshort{lstm} and \acrshort{lstmgru}â€”
due to its ability to preserve the shape of the original data while constraining it within a range that accelerates 
convergence and improves training stability.

\begin{table}[H]
\centering
\caption{Feature scaling methods used in LSTM-based stock prediction studies.}
\label{tab:feature-scaling}
\begin{tabular}{p{3.5cm}p{3.2cm}p{6.8cm}} \hline
\textbf{Study} & \textbf{Scaling Method} & \textbf{Purpose / Notes} \\ \hline\hline
\parencite{shaban2024SMPDL} & Min-Max Normalization & Normalization was used to reduce variance and improve model performance in minute-level LSTM-BiGRU forecasting. \\
\parencite{nabipour2020DeepLearning} & Z-score Standardization & Feature scaling was part of the preprocessing pipeline to manage multiple indicators across different ranges.  \\
\parencite{phuoc2024StockPrediction} & Min-Max Normalization & Min-max scaling was applied across features to train multiple deep models on structured market data.  \\
\hline
\end{tabular}
\end{table}

\subsection{Model Selection}

Selecting the best model for stock price prediction is a complex task that depends on
multiple factors, including data complexity, computational efficiency, and forecasting 
accuracy. \acrshortpl{rnn} have been widely used due to their ability to capture 
time-series dependencies and handle sequential stock market data 
effectively~\parencite{shaban2024SMPDL}.

Recent research suggests that hybrid models combining \acrshort{lstm} with 
\acrshort{gru} or \acrshort{bigru} provide superior performance compared to standalone
architectures. \acrshort{lstmgru} balances efficiency and predictive accuracy, while 
\acrshort{lstmbigru} achieves the highest accuracy by capturing both past and future
dependencies~\parencite{chang2024StockPrediction}. Table~\ref{tab:stock_models} summarizes the key advantages and trade-offs of these models, based on findings from deep learning applied to financial forecasting.

\begin{table}[H]
    \centering
    \caption{Comparison of Deep Learning Models for Stock Price Prediction}
    \label{tab:stock_models}
    \begin{tabular}{cp{4cm}p{4cm}p{4cm}}
        \hline
        \textbf{Model} & \textbf{Best For} & \textbf{Pros} & \textbf{Cons} \\
        \hline
        \acrshort{lstm} & Long-sequence dependencies, fundamental analysis & Captures
        long-term dependencies effectively, reduces vanishing gradient
        issues~\parencite{shaban2024SMPDL}. & High memory consumption, slower training 
        compared to GRU~\parencite{chang2024StockPrediction}. \\
        \acrshort{gru} & Real-time predictions, medium-term forecasting & Requires 
        fewer parameters, trains faster than LSTM while maintaining similar
        accuracy~\parencite{guo2024LSTMStock}. & May lose some long-term dependencies 
        compared to LSTM. \\
        \acrshort{bigru} & Volatile stocks, capturing full sequence trends & Processes 
        data in both forward and backward directions, leading to better sequence
        learning~\parencite{shaban2024SMPDL}. & Higher computational cost due to 
        bidirectional processing. \\
        \acrshort{lstmgru} & Faster training, mid-term forecasting & 
        Combines \acrshort{lstm}'s memory retention with \acrshort{gru}â€™s efficiency,
        achieving strong performance with reduced training 
        time~\parencite{chang2024StockPrediction}. & May not capture as much detail as 
        \acrshort{bigru} in highly volatile markets. \\
        \acrshort{lstmbigru} & High-accuracy stock forecasting, volatile markets & 
        Captures both forward and backward dependencies, significantly improving 
        predictive accuracy in volatile stocks~\parencite{shaban2024SMPDL}. & 
        Higher computational cost compared to \acrshort{lstmgru}, requiring more 
        processing power. \\ \hline
    \end{tabular}
\end{table}

For \acrshort{wday} stock prediction, the approach will involve implementing and evaluating 
two deep learning models: \acrshort{lstm}, and \acrshort{lstmbigru}.
These models are selected due to their ability to capture long-term dependencies, 
short-term trends, and bidirectional patterns in stock market data.

\subsection{Hyperparameter tuning for the model }

Table~\ref{tab:hyperparamsconf} summarizes the hyperparameter configurations 
reported in recent \acrshort{lstm}-based stock prediction studies. Common values include a 
sequence length of 60, dropout rates around 0.2, and hidden unit sizes ranging from 50 to 
128. Based on this, key parameters such as sequence length, number of hidden units, dropout
rate, and learning rate will be explored and tuned in this study to identify the 
best-performing configuration for next-day price prediction.

\begin{table}[H]
\centering
\caption{Hyperparameter Recommendations for LSTM Models}
\label{tab:hyperparamsconf}
\begin{tabular}{ccp{6.5cm}}
\hline
\textbf{Hyperparameter} & \textbf{Recommended Range} & \textbf{Description} \\ \hline\hline
Learning Rate & 0.001 - 0.0001 & Controls step size during optimization~\parencite{parmar2018stock}. 
Smaller learning rates ensure smoother convergence but increase training time.\\
Hidden Layer Units & 16-128 & More units increase model complexity and potentially accuracy but risk overfitting \\
Batch Size & 32-128 & Affects training stability and convergence \\
Sequence Length & 30-90 & Longer sequences capture more historical context but require more resources. \\
Optimizer & Adam, RMSprop & Commonly used for recurrent networks \\
Dropout Rate & 0.2 - 0.5 & Prevents overfitting~\parencite{agrawal2022StockPrediction} \\
Number of Epochs & 50 - 200 & Determines training iterations \\
Activation Function & Tanh, ReLU & Tanh preferred for LSTMs \\
Early Stopping & Patience = 1 & Prevents unnecessary computation by halting when validation loss 
stops improving~\parencite{chang2024StockPrediction} \\
\hline
\end{tabular}
\end{table}

To effectively identify the optimal set of hyperparameters for both \acrshort{lstm} and hybrid \acrshort{lstmgru} models, 
this study employs \emph{Random Search} as the primary tuning method. Rather than exhaustively testing every possible 
combination (as in Grid Search), Random Search samples a wide range of parameter values randomly across the defined search 
space. This approach has been shown to be more computationally efficient while still providing high-quality solutions,
especially when dealing with high-dimensional hyperparameter spaces~\parencite{bergstra2012random}.

The search is conducted over the hyperparameter ranges listed in Table~\ref{tab:hyperparamsconf}, including learning rate, 
batch size, dropout rate, number of epochs, and optimizer. The goal is to find the configuration that minimizes 
validation loss and generalizes well across different market conditions. This tuning process plays a key role in 
ensuring model stability, improving convergence, and enhancing next-day prediction accuracy.

\subsection{Performance and evaluation}

The selection of evaluation metrics is based on established research in stock market 
forecasting~\parencite{agrawal2022StockPrediction, nabipour2020DeepLearning, guo2024LSTMStock}:

\begin{description}
\item[\acrfull{mae}] Represents the average absolute difference between predicted and 
                     actual stock prices.
\item[\acrfull{mse}] Highlights prediction errors with a quadratic penalty, making it 
                     sensitive to larger deviations.
\item[\acrfull{r2}] Indicates how well the model explains the variance in the data, with values 
                    closer to 1 denoting better performance.
\end{description}

To ensure optimal performance, the process will include:
\begin{itemize}
    \item Properly scaling the input features using an appropriate scaling technique.
    \item Searching for the best hyperparameters to optimize learning rate, batch size, number of layers, and hidden units.
    \item Evaluating model performance.
\end{itemize}
