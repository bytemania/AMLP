\clearpage
\section{Methodology}

This methodology follows established approaches in stock market prediction using deep
learning models, drawing from previous research in the 
field~\parencite{parmar2018stock, nabipour2020DeepLearning,
chang2024StockPrediction,agrawal2022StockPrediction, shaban2024SMPDL}. 

We present here the step-by-step process undertaken to build, train, and evaluate deep
learning models for \acrshort{wday} stock price prediction, ensuring that each methodological
decision is justified based on prior studies or logical reasoning.

\subsection{Methodology workflow}

\begin{figure}[H]
    \centering
    \caption{Workflow for Stock Market Prediction using LSTM and related models}
    \label{fig:modelwf}
    \begin{tikzpicture}[node distance=1.5cm]
        \node (dataset) [startstop] {Twelve Stock Market Dataset};
        \node (api) [process, below of=dataset] {\textbf{Fetch WDAY Stock Prices and Indicators}};
        \node (preprocessing) [process, below of=api] {\textbf{Data Preprocessing}};
        \node (learning) [process, below of=preprocessing] {\textbf{Learning Algorithms}};
        \node (hyperparam) [process, right of=learning, xshift=3.5cm] {\textbf{Hyperparameter Tuning}};
        \node (validation) [process, below of =learning] {\textbf{Cross Validation}};
        \node (performance) [process, below of=validation] {\textbf{Performance Measures}};
        \node (comparison) [process, below of=performance] {\textbf{Comparison of Results}};
    
        \draw [arrow] (dataset.south) -- (api.north);
        \draw [arrow] (api.south) -- (preprocessing.north);
        \draw [arrow] (preprocessing.south) -- (learning.north);
        \draw [arrow] (hyperparam.west) --(learning.east);
        \draw [arrow] (learning.south) -- (validation.north);
        \draw [arrow] (validation.south) -- (performance.north);
        \draw [arrow] (performance.south) -- (comparison.north);
    \end{tikzpicture}
\end{figure}

The diagram (Figure~\ref{fig:modelwf}) presents a structured workflow for stock market prediction 
using \acrshort{lstm} and related deep learning models. The workflow is divided into key stages that
ensure a systematic approach to data processing, training, evaluation, and comparison. Below is a 
step-by-step breakdown of each component:

\begin{description}
    \item[Twelve Stock Market Dataset] The process begins with collecting stock market data from
         twelve different markets. This dataset serves as the foundation for training and evaluation
    \item[Fetch WDAY Stock Prices and Indicators]  From the dataset, \acrshort{wday} stock prices 
         and technical indicators are extracted for predictive modeling.
    \item[Data Preprocessing] Raw stock price data undergoes cleaning, normalization, and 
         transformation to ensure compatibility with machine learning models.
    \item[Learning Algorithms] The \acrshort{lstm} models are trained using the processed dataset.
    \item[Hyperparameter Tuning] Hyperparameter tuning is performed in parallel to optimize
        learning rates, batch sizes, dropout rates, and activation functions for better performance.
    \item[Cross-Validation] The models are validated using cross-validation techniques to assess 
         generalization ability and reduce overfitting.     
    \item[Performance Measures] The trained models are evaluated using metrics such as 
         \acrshort{rmse}, \acrshort{mae}, \acrshort{mse}, and \acrshort{r2} to determine 
         prediction accuracy.
    \item[Comparison of Results] The final step involves comparing the performance of 
         different models and identifying the best-performing approach for stock price forecasting.
\end{description}

\subsection{Data Collection}

Stock market prediction research has demonstrated that historical price trends, combined 
with technical indicators, improve forecasting accuracy~\parencite{guo2024LSTMStock,
agrawal2022StockPrediction,balasubramanian2023SystematicSurvey,phuoc2024StockPrediction}. 

The study leverages historical stock price data for \acrshort{wday}, obtained from the
Twelve Stock Market API. This dataset includes daily open, high, low, close, and volume prices. In
addition, the study incorporates multiple technical indicators, to
enhance predictive accuracy~\parencite{parmar2018stock, nabipour2020DeepLearning,
guo2024LSTMStock}. 

We will organize our indicators into distinct categories, each designed to quantify a unique aspect that contributes to the final stock price:

\begin{description}
    \item[Pricing Data] Includes daily open, high, low, close, and volume (OHLCV) values,
    which serve as the foundation for tracking stock performance.
    \item[Trend Indicators] Measure the overall direction of the market, helping identify 
    whether a stock is in an uptrend, downtrend, or moving sideways.
    \item[Momentum Indicators] Evaluate the speed and strength of price movements, providing
    insights into potential trend reversals and whether the stock is overbought or oversold.
    \item[Volatility Indicators] Assess market risk and price fluctuations, helping traders 
    identify potential breakouts and periods of stability.
    \item[Volume Indicators] Analyze trading activity to confirm price trends, showing
    whether investors are accumulating (buying) or distributing (selling) a stock.
\end{description}

A detailed overview of the dataset is available in Appendix~\ref{app:dataset}.

\subsection{Data Preprocessing}

\subsubsection{Timeframe and frequency}

Using a 10‐year window with daily sampling for \acrshort{wday} stocks would yield roughly 
252 datapoints per year (approximately 2,500 total), which might be too sparse for a complex model 
and could lead to underfitting\footnote{Underfitting occurs when the model is too simple relative to the complexity of the data, failing to capture the underlying market dynamics.}. Since our study focuses
exclusively on \acrshort{wday} stocks, we can increase the data density by sampling \emph{every hour 
during trading hours}. This strategy boosts the number of data points by a factor of 6–8 per trading
day, allowing us to capture more nuanced intraday trends and volatility.

For a comprehensive comparison of different timeframes and frequencies, please refer to 
Appendix~\ref{app:timeframes}, which summarizes these trade-offs based on previous studies.

In summary, \acrshort{wday} stock pricing data will be collected from \emph{2014 to 2024} with hourly 
sampling during trading hours. Although the majority of studies recommend daily frequency, our 
approach mitigates the risk of underfitting by significantly increasing the dataset size.

\subsubsection{Feature transformation}

Since stock market data is inherently \emph{time-series based}, the date column must be properly transformed 
into a \emph{datetime} format and set as the dataset's \emph{index}. This transformation is crucial for 
ensuring temporal consistency and optimizing analytical procedures. Proper datetime formatting enables 
various time-series-specific operations, such as trend identification and seasonal pattern detection, which 
are fundamental for accurate forecasting models~\parencite{chang2024StockPrediction}.

By setting the date as an index, the dataset benefits in multiple ways:
\begin{description}
    \item[Chronological Integrity] Ensures that stock prices are analyzed in the correct order,
    avoiding inconsistencies in sequential learning models like LSTM and GRU~\parencite{guo2024LSTMStock}.
    \item[Enhanced Feature Extraction] Facilitates time-based calculations such as moving averages, 
    momentum, and rolling-window statistics that provide deeper insights into stock 
    trends~\parencite{shaban2024SMPDL}.
    \item[Efficient Resampling] Enables aggregation or decomposition of data into different time 
    intervals, such as hourly, daily, or weekly periods, which can be beneficial for various prediction 
    models~\parencite{agrawal2022StockPrediction}.
\end{description} 

\subsubsection{Feature Selection Based}

Feature selection is a critical preprocessing step in stock market prediction models. Highly 
correlated features introduce redundancy, increase model complexity, and may lead to 
multicollinearity, negatively impacting predictive 
performance~\parencite{balasubramanian2023SystematicSurvey, guo2024LSTMStock}. By analyzing the
correlation matrix, we identify and remove features that exhibit excessive 
correlation, ensuring a more robust and interpretable model.

\paragraph{Correlation Matrix and Redundancy Removal}
The correlation matrix provides a quantitative measure of the relationships between different
technical indicators. If two features have a correlation coefficient \(\rho > 0.9\), they
contain almost identical information~\parencite{nabipour2020DeepLearning}. In such cases, one of
the two features can be removed to prevent redundancy.

\paragraph{Criteria for Feature Selection}
To systematically reduce feature redundancy, we follow these steps:
\begin{enumerate}
    \item Compute the correlation matrix for all numerical features in the dataset.
    \item Identify feature pairs with a correlation coefficient \(\rho > 0.9\).
    \item Retain one feature from each highly correlated pair based on domain relevance or predictive importance.
    \item Remove the redundant feature to improve model efficiency and prevent overfitting.
\end{enumerate}

\subsubsection{Dataset split}

In financial forecasting, time-based dataset splitting is critical to
preserving the sequential nature of stock market data. Unlike traditional
machine learning datasets, where random splitting can be applied, time-series
data must maintain chronological integrity to reflect real-world prediction
challenges. Several studies emphasize the importance of time-aware partitioning
in stock price prediction, ensuring that past data informs future forecasts
without data leakage.

For stock price forecasting, the dataset is divided as described 
in Table~\ref{tab:dataset_split}.

\begin{table}[H]
    \centering
    \caption{Time-Based Dataset Splitting Strategy}
    \label{tab:dataset_split}
    \begin{tabular}{ccp{9cm}}
        \hline
        \textbf{Dataset} & \textbf{Percentage} & \textbf{Description} \\
        \hline\hline
        Training Set & 70\% & Contains the earliest stock
        price data, allowing the model to learn long-term historical patterns. \\
        Validation Set & 15\% & Consists of the data immediately following 
        the training period, used for hyperparameter tuning. \\
        Test Set & 15\% & Represents the most recent stock prices, ensuring that 
        evaluation occurs on completely unseen future data. \\
        \hline
    \end{tabular}
\end{table}

This partitioning aligns with best practices in financial machine learning, 
where an 80/20 or 70/15/15 split balances the need for a sufficiently large 
training set while keeping a meaningful test set for 
evaluation~\parencite{chang2024StockPrediction}.

\paragraph{How the Data is Split Over Time}
As shown in Equation~\ref{eq:dataset_split}, the dataset is divided into three sequential parts to maintain chronological integrity.

Let $D={X_1,X_2,\cdots,X_T}$ be the full dataset where $X_t$ represents the stock market data at time $t$. 

\begin{equation}
\label{eq:dataset_split}
    \underbrace{X_1, X_2, \dots, X_{T_{\text{train}}}}_{\text{Training Data (70\%)}} \quad
    \underbrace{X_{T_{\text{train}}+1}, \dots, X_{T_{\text{val}}}}_{\text{Validation Data (15\%)}} \quad
    \underbrace{X_{T_{\text{val}}+1}, \dots, X_T}_{\text{Test Data (15\%)}} 
\end{equation}

This strict time-based split ensures that future data is not used to train or adjust the model, maintaining realistic forecast conditions\parencite{guo2024LSTMStock}.

\subsubsection{Feature Scaling}

Feature scaling is an essential preprocessing step in deep learning models, particularly for
time-series forecasting in stock market prediction. Models such as \acrshort{lstm}, 
\acrshort{lstmgru}, and \acrshort{lstmbigru} require properly scaled inputs to ensure stable
training, faster convergence, and improved predictive accuracy. The choice of a scaling
method significantly impacts model performance, as improper scaling can lead to gradient
instability, slow learning, or biased weight updates~\parencite{chang2024StockPrediction}.

For \acrshortpl{rnn} like \acrshort{lstm} and its variants, the distribution 
and \emph{range of input features must align with activation functions such as sigmoid and tanh}. Based on the literature, 
\emph{Min-Max Scaling, Z-Score Standardization, and Robust Scaling} are the most
commonly used methods, each serving a different purpose depending on the model 
architecture~\parencite{balasubramanian2023SystematicSurvey}.

Table~\ref{tab:scaling_methods} presents a comparison of the most effective feature scaling techniques for LSTM-based models, highlighting their suitability and theoretical advantages.

\begin{table}[H]
    \centering
    \caption{Comparison of Feature Scaling Methods for LSTM-Based Models}
    \label{tab:scaling_methods}
    \begin{tabular}{ccp{7.5cm}}
        \hline
        \textbf{Scaling Method} & \textbf{Best for} & \textbf{Justification} \\
        \hline\hline
        Min-Max Scaling & \acrshort{lstm} & Preserves the relative magnitude of stock price 
        movements and ensures numerical stability during training. Ideal for 
        sequential models with long-term 
        dependencies~\parencite{chang2024StockPrediction}. \\
        Z-Score Standardization & \acrshort{lstmgru} & Normalizes features to have zero mean
        and unit variance, stabilizing training and improving hybrid model 
        convergence~\parencite{balasubramanian2023SystematicSurvey}. \\
        Robust Scaling & \acrshort{lstmbigru} & Handles outliers effectively, preventing 
        distortions in bidirectional recurrent networks that analyze both past and 
        future dependencies~\parencite{shaban2024SMPDL}. \\ \hline
    \end{tabular}
\end{table}

\subsection{Model Selection}

Selecting the best model for stock price prediction is a complex task that depends on
multiple factors, including data complexity, computational efficiency, and forecasting 
accuracy. \acrshortpl{rnn} have been widely used due to their ability to capture 
time-series dependencies and handle sequential stock market data 
effectively~\parencite{shaban2024SMPDL}.

Recent research suggests that hybrid models combining \acrshort{lstm} with 
\acrshort{gru} or \acrshort{bigru} provide superior performance compared to standalone
architectures. \acrshort{lstmgru} balances efficiency and predictive accuracy, while 
\acrshort{lstmbigru} achieves the highest accuracy by capturing both past and future
dependencies~\parencite{chang2024StockPrediction}. Table xx summarizes the key advantages and trade-offs of these models, based on findings from deep learning applied to financial forecasting.

\begin{table}[H]
    \centering
    \caption{Comparison of Deep Learning Models for Stock Price Prediction}
    \label{tab:stock_models}
    \begin{tabular}{cp{4cm}p{4cm}p{4cm}}
        \hline
        \textbf{Model} & \textbf{Best For} & \textbf{Pros} & \textbf{Cons} \\
        \hline
        \acrshort{lstm} & Long-sequence dependencies, fundamental analysis & Captures
        long-term dependencies effectively, reduces vanishing gradient
        issues~\parencite{shaban2024SMPDL}. & High memory consumption, slower training 
        compared to GRU~\parencite{chang2024StockPrediction}. \\
        \acrshort{gru} & Real-time predictions, medium-term forecasting & Requires 
        fewer parameters, trains faster than LSTM while maintaining similar
        accuracy~\parencite{guo2024LSTMStock}. & May lose some long-term dependencies 
        compared to LSTM. \\
        \acrshort{bigru} & Volatile stocks, capturing full sequence trends & Processes 
        data in both forward and backward directions, leading to better sequence
        learning~\parencite{shaban2024SMPDL}. & Higher computational cost due to 
        bidirectional processing. \\
        \acrshort{lstmgru} & Faster training, mid-term forecasting & 
        Combines \acrshort{lstm}'s memory retention with \acrshort{gru}’s efficiency,
        achieving strong performance with reduced training 
        time~\parencite{chang2024StockPrediction}. & May not capture as much detail as 
        \acrshort{bigru} in highly volatile markets. \\
        \acrshort{lstmbigru} & High-accuracy stock forecasting, volatile markets & 
        Captures both forward and backward dependencies, significantly improving 
        predictive accuracy in volatile stocks~\parencite{shaban2024SMPDL}. & 
        Higher computational cost compared to \acrshort{lstmgru}, requiring more 
        processing power. \\ \hline
    \end{tabular}
\end{table}

For \acrshort{wday} stock prediction, the approach will involve implementing and evaluating 
three deep learning models: \acrshort{lstm}, \acrshort{lstmgru}, and \acrshort{lstmbigru}.
These models are selected due to their ability to capture long-term dependencies, 
short-term trends, and bidirectional patterns in stock market data.

To ensure optimal performance, the process will include:
\begin{itemize}
    \item Properly scaling the input features using an appropriate scaling technique.
    \item Searching for the best hyperparameters to optimize learning rate, batch size, number of layers, and hidden units.
    \item Evaluating model performance.
\end{itemize}

\subsection{Performance and evaluation}

The selection of evaluation metrics is based on established research in stock market 
forecasting~\parencite{agrawal2022StockPrediction, nabipour2020DeepLearning, guo2024LSTMStock}:

\begin{description}
\item[\acrfull{rmse}] Measures the standard deviation of residuals between predicted and 
                      actual values. Lower values indicate better predictive performance.
\item[\acrfull{mae}] Represents the average absolute difference between predicted and 
                     actual stock prices.
\item[\acrfull{mse}] Highlights prediction errors with a quadratic penalty, making it 
                     sensitive to larger deviations.
\item[\acrfull{r2}] Indicates how well the model explains the variance in the data, with values 
                    closer to 1 denoting better performance.
\end{description}




